# Question-Answering-with-a-Fine-Tuned-BERT
Question Answering is a computer science discipline within the fields of information retrieval and natural language processing, which focuses on building systems that automatically answer questions posed by humans in a natural language. One of the biggest challenges in NLP is the lack of enough training data. Overall, there is enormous amount of text data available, but if we want to create task-specific datasets, we need to split that pile into the very many diverse fields. Deep learning models tend to perform exceptionally well when they are trained on millions, or billions, of annotated training examples. To solve this problem researchers at google came up with pretrained model like BERT (Bidirectional Encoder Representations from Transformer) using various techniques for training general purpose language representation models using the enormous amount of unannotated text on the web. Bert is trained on corpus of Wikipedia and books. These general purpose pre-trained models can then be fine-tuned on smaller task-specific datasets. SquAD dataset has been used for question answering task by finetuning on BERT and has given state of the art results. It consists of title, paragraph which contains context, Qas. Qas contains information about answers, answers start and id. In our study we want to examine if this pre trained models which are trained on general corpus do really perform well or do they need to be trained on task specific dataset. In this project we are using pretrained general language models trained on large amount of data and finetuning them on domain specific dataset for Q/A task.